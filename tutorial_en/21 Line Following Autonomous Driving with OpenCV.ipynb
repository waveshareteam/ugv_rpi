{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "019fbb00-544a-4870-aab5-5e1369e3bac6",
   "metadata": {},
   "source": [
    "# Line Following Autonomous Driving with OpenCV\n",
    "\n",
    "In this tutorial, we'll use basic functionalities of OpenCV to detect yellow lines (default color) in the image and control the direction of the chassis based on the position of these lines. Please note that in this example, the chassis won't move. Instead, we'll only showcase the algorithms using OpenCV on the image. For safety reasons, we won't integrate motion control in this tutorial, as it's heavily influenced by external factors. Users should fully understand the code's functionality before adding corresponding motion control features.\r\n",
    "\r\n",
    "If you want to control the robot's movement through this example, please refer to the \"Python Chassis Motion Control\" section to add the relevant motion control functions (our open-source example is located in robot_ctrl.py)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ecdb3f7e-09a4-4e54-87dd-b59cb1fd3deb",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "## Preparation\n",
    "\n",
    "Since the product automatically runs the main program at startup, which occupies the camera resource, this tutorial cannot be used in such situations. You need to terminate the main program or disable its automatic startup before restarting the robot.\n",
    "\n",
    "It's worth noting that because the robot's main program uses multi-threading and is configured to run automatically at startup through crontab, the usual method sudo killall python typically doesn't work. Therefore, we'll introduce the method of disabling the automatic startup of the main program here.\n",
    "\n",
    "### Terminate the Main Program\n",
    "\n",
    "1. Click the \"+\" icon next to the tab for this page to open a new tab called \"Launcher.\"\n",
    "2. Click on \"Terminal\" under \"Other\" to open a terminal window.\n",
    "3. Type bash into the terminal window and press Enter.\n",
    "4. Now you can use the Bash Shell to control the robot.\n",
    "5. Enter the command: crontab -e.\n",
    "6. If prompted to choose an editor, enter 1 and press Enter to select nano.\n",
    "7. After opening the crontab configuration file, you'll see the following two lines:\n",
    "> @reboot ~/ugv_pt_rpi/ugv-env/bin/python ~/ugv_pt_rpi/app.py >> ~/ugv.log 2>&1\n",
    ">\n",
    ">@reboot /bin/bash ~/ugv_pt_rpi/start_jupyter.sh >> ~/jupyter_log.log 2>&1\n",
    "\n",
    "8. Add a # character at the beginning of the line with ……app.py >> …… to comment out this line.\n",
    "> #@reboot ~/ugv_pt_rpi/ugv-env/bin/python ~/ugv_pt_rpi/app.py >> ~/ugv.log 2>&1\n",
    ">\n",
    ">@reboot /bin/bash ~/ugv_pt_rpi/start_jupyter.sh >> ~/jupyter_log.log 2>&1\n",
    "\n",
    "9. Press Ctrl + X in the terminal window to exit. It will ask you Save modified buffer? Enter Y and press Enter to save the changes.\n",
    "10. Reboot the device. Note that this process will temporarily close the current Jupyter Lab session. If you didn't comment out ……start_jupyter.sh >>…… in the previous step, you can still use Jupyter Lab normally after the robot reboots (JupyterLab and the robot's main program app.py run independently). You may need to refresh the page.\n",
    "11. One thing to note is that since the lower machine continues to communicate with the upper machine through the serial port, the upper machine may not start up properly during the restart process due to the continuous change of serial port levels. Taking the case where the upper machine is a Raspberry Pi, after the Raspberry Pi is shut down and the green LED is constantly on without the green LED blinking, you can turn off the power switch of the robot, then turn it on again, and the robot will restart normally.\n",
    "12. Enter the reboot command: sudo reboot.\n",
    "13. After waiting for the device to restart (during the restart process, the green LED of the Raspberry Pi will blink, and when the frequency of the green LED blinking decreases or goes out, it means that the startup is successful), refresh the page and continue with the remaining part of this tutorial."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f373291f-9bf0-4182-be22-7068a3740ed5",
   "metadata": {},
   "source": [
    "## Example\n",
    "\n",
    "The following code block can be run directly:\n",
    "\n",
    "1. Select the code block below.\n",
    "2. Press Shift + Enter to run the code block.\n",
    "3. Watch the real-time video window.\n",
    "4. Press STOP to close the real-time video and release the camera resources.\n",
    "\n",
    "### If you cannot see the real-time camera feed when running:\n",
    "\n",
    "- Click on Kernel -> Shut down all kernels above.\n",
    "- Close the current section tab and open it again.\n",
    "- Click `STOP` to release the camera resources, then run the code block again.\n",
    "- Reboot the device.\n",
    "\n",
    "### Features of this Section\n",
    "\n",
    "After running the following code block, you can place a yellow tape in front of the camera and observe if there are contours of the yellow tape in the black screen. Try to detect the yellow tape using two target detection lines."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7ec0096c-d92d-49fa-85a3-aeba8f844f05",
   "metadata": {},
   "outputs": [],
   "source": [
    "import cv2  # Import the OpenCV library for image processing\n",
    "import imutils, math  # Auxiliary libraries for image processing and mathematical operations\n",
    "from picamera2 import Picamera2  # Library for accessing the Raspberry Pi Camera\n",
    "import numpy as np\n",
    "from IPython.display import display, Image  # Library for displaying images in Jupyter Notebook\n",
    "import ipywidgets as widgets  # Library for creating interactive widgets such as buttons\n",
    "import threading  # Library for creating new threads to execute tasks asynchronously\n",
    "\n",
    "# Stop button\n",
    "# ================\n",
    "stopButton = widgets.ToggleButton(\n",
    "    value=False,\n",
    "    description='STOP',\n",
    "    disabled=False,\n",
    "    button_style='danger',  # Button style: 'success', 'info', 'warning', 'danger', or ''\n",
    "    tooltip='Description',\n",
    "    icon='square'  # FontAwesome icon name (without the `fa-` prefix)\n",
    ")\n",
    "\n",
    "# Line Tracking Autonomous Driving\n",
    "\n",
    "# Upper detection line, 0.6 represents position, with higher values indicating farther from the bottom\n",
    "sampling_line_1 = 0.6\n",
    "\n",
    "# Lower detection line, the value needs to be greater than sampling_line_1 and less than 1\n",
    "sampling_line_2 = 0.9\n",
    "\n",
    "# Impact of line slope on turning\n",
    "slope_impact = 1.5\n",
    "\n",
    "# Impact of line position detected by the lower detection line on turning\n",
    "base_impact = 0.005\n",
    "\n",
    "# Impact of current speed on turning\n",
    "speed_impact = 0.5\n",
    "\n",
    "# Line tracking speed\n",
    "line_track_speed = 0.3\n",
    "\n",
    "# Impact of slope on line tracking speed\n",
    "slope_on_speed = 0.1\n",
    "\n",
    "# Target line color, in HSV color space\n",
    "line_lower = np.array([25, 150, 70])\n",
    "line_upper = np.array([42, 255, 255])\n",
    "\n",
    "def view(button):\n",
    "    # If you are using a CSI camera, uncomment the picam2 related code below, \n",
    "    # and comment out the camera related code.\n",
    "    # This is because the latest version of OpenCV (4.9.0.80) no longer supports CSI cameras, \n",
    "    # and you need to use picamera2 to capture camera images.\n",
    "    \n",
    "    # picam2 = Picamera2()\n",
    "    # picam2.configure(picam2.create_video_configuration(main={\"format\": 'XRGB8888', \"size\": (640, 480)}))\n",
    "    # picam2.start()\n",
    "\n",
    "    camera = cv2.VideoCapture(-1) \n",
    "    camera.set(cv2.CAP_PROP_FRAME_WIDTH, 640)\n",
    "    camera.set(cv2.CAP_PROP_FRAME_HEIGHT, 480)\n",
    "    \n",
    "    display_handle=display(None, display_id=True)\n",
    "    \n",
    "    while True:\n",
    "        # img = picam2.capture_array()\n",
    "        _, img = camera.read()\n",
    "        # frame = cv2.flip(frame, 1) # if your camera reverses your image\n",
    "\n",
    "        # frame = cv2.cvtColor(frame, cv2.COLOR_BGR2RGB)\n",
    "\n",
    "        height, width = img.shape[:2]\n",
    "        center_x, center_y = width // 2, height // 2\n",
    "        # Image preprocessing, including color space conversion, Gaussian blur, color range filtering, etc.\n",
    "        hsv = cv2.cvtColor(img, cv2.COLOR_BGR2HSV)\n",
    "\n",
    "        line_mask = cv2.inRange(hsv, line_lower, line_upper)  # Filter out target lines based on color range\n",
    "        line_mask = cv2.erode(line_mask, None, iterations=1)  # Erosion operation to remove noise\n",
    "        line_mask = cv2.dilate(line_mask, None, iterations=1)  # Dilation operation to enhance target lines\n",
    "\n",
    "        # Detect target lines based on the positions of the upper and lower sampling lines, and calculate steering and speed control signals based on the detection results\n",
    "        sampling_h1 = int(height * sampling_line_1)\n",
    "        sampling_h2 = int(height * sampling_line_2)\n",
    "        \n",
    "        get_sampling_1 = line_mask[sampling_h1]\n",
    "        get_sampling_2 = line_mask[sampling_h2]\n",
    "\n",
    "        # Calculate the width of the target line at the upper and lower sampling lines\n",
    "        sampling_width_1 = np.sum(get_sampling_1 == 255)\n",
    "        sampling_width_2 = np.sum(get_sampling_2 == 255)\n",
    "\n",
    "        if sampling_width_1:\n",
    "            sam_1 = True\n",
    "        else:\n",
    "            sam_1 = False\n",
    "        if sampling_width_2:\n",
    "            sam_2 = True\n",
    "        else:\n",
    "            sam_2 = False\n",
    "\n",
    "        # Get the edge indices of the target line at the upper and lower sampling lines\n",
    "        line_index_1 = np.where(get_sampling_1 == 255)\n",
    "        line_index_2 = np.where(get_sampling_2 == 255)\n",
    "\n",
    "        # If the target line is detected at the upper sampling line, calculate the center position of the target line\n",
    "        if sam_1:\n",
    "            sampling_1_left  = line_index_1[0][0]  # The leftmost index of the target line at the upper sampling line\n",
    "            sampling_1_right = line_index_1[0][sampling_width_1 - 1]  # The rightmost index of the target line at the upper sampling line\n",
    "            sampling_1_center= int((sampling_1_left + sampling_1_right) / 2)  # The index of the center of the target line at the upper sampling line\n",
    "        # If the target line is detected at the lower sampling line, calculate the center position of the target line\n",
    "        if sam_2:\n",
    "            sampling_2_left  = line_index_2[0][0]\n",
    "            sampling_2_right = line_index_2[0][sampling_width_2 - 1]\n",
    "            sampling_2_center= int((sampling_2_left + sampling_2_right) / 2)\n",
    "\n",
    "        # Initialize steering and speed control signals\n",
    "        line_slope = 0\n",
    "        input_speed = 0\n",
    "        input_turning = 0\n",
    "        \n",
    "        # If the target line is detected at both sampling lines, calculate the line slope and speed and steering control signals based on the slope and position of the target line\n",
    "        if sam_1 and sam_2:\n",
    "            line_slope = (sampling_1_center - sampling_2_center) / abs(sampling_h1 - sampling_h2) # Calculate the line slope\n",
    "            impact_by_slope = slope_on_speed * abs(line_slope) # Calculate the impact on speed based on the slope\n",
    "            input_speed = line_track_speed - impact_by_slope # Calculate the speed control signal\n",
    "            input_turning = -(line_slope * slope_impact + (sampling_2_center - center_x) * base_impact) # Calculate the steering control signal\n",
    "        elif not sam_1 and sam_2: # If only the target line is detected at the lower sampling line\n",
    "            input_speed = 0 # Set speed to 0\n",
    "            input_turning = (sampling_2_center - center_x) * base_impact # Calculate the steering control signal\n",
    "        elif sam_1 and not sam_2: # If only the target line is detected at the upper sampling line\n",
    "            input_speed = (line_track_speed / 3) # Slow down speed\n",
    "            input_turning = 0 # No steering\n",
    "        else: # If no target line is detected at both sampling lines\n",
    "            input_speed = - (line_track_speed / 3) # Reverse\n",
    "            input_turning = 0 # No steering\n",
    "\n",
    "        # base.base_json_ctrl({\"T\":13,\"X\":input_speed,\"Z\":input_turning})\n",
    "\n",
    "        cv2.putText(line_mask, f'X: {input_speed:.2f}, Z: {input_turning:.2f}', (center_x+50, center_y+0), cv2.FONT_HERSHEY_SIMPLEX, 0.5, (255, 255, 255), 1)\n",
    "        # Visualization operations, including drawing lines at sampling line positions, marking sampling results, and displaying steering and speed control signals\n",
    "        cv2.line(line_mask, (0, sampling_h1), (img.shape[1], sampling_h1), (255, 0, 0), 2)\n",
    "        cv2.line(line_mask, (0, sampling_h2), (img.shape[1], sampling_h2), (255, 0, 0), 2)\n",
    "\n",
    "        if sam_1:\n",
    "            # Draw green marker lines at both ends of the target line at the upper sampling line\n",
    "            cv2.line(line_mask, (sampling_1_left, sampling_h1+20), (sampling_1_left, sampling_h1-20), (0, 255, 0), 2)\n",
    "            cv2.line(line_mask, (sampling_1_right, sampling_h1+20), (sampling_1_right, sampling_h1-20), (0, 255, 0), 2)\n",
    "        if sam_2:\n",
    "            # Draw green marker lines at both ends of the target line at the lower sampling line\n",
    "            cv2.line(line_mask, (sampling_2_left, sampling_h2+20), (sampling_2_left, sampling_h2-20), (0, 255, 0), 2)\n",
    "            cv2.line(line_mask, (sampling_2_right, sampling_h2+20), (sampling_2_right, sampling_h2-20), (0, 255, 0), 2)\n",
    "        if sam_1 and sam_2:\n",
    "            # If the target line is detected at both upper and lower sampling lines, draw a red line from the center of the upper sampling line to the center of the lower sampling line\n",
    "            cv2.line(line_mask, (sampling_1_center, sampling_h1), (sampling_2_center, sampling_h2), (255, 0, 0), 2)\n",
    "        \n",
    "        _, frame = cv2.imencode('.jpeg', line_mask)\n",
    "        display_handle.update(Image(data=frame.tobytes()))\n",
    "        if stopButton.value==True:\n",
    "            # picam2.close() # If yes, close the camera\n",
    "            cv2.release() # If yes, close the camera\n",
    "            display_handle.update(None)\n",
    "\n",
    "\n",
    "# Display the \"STOP\" button and start a thread to execute the display function\n",
    "# ================\n",
    "display(stopButton)\n",
    "thread = threading.Thread(target=view, args=(stopButton,))\n",
    "thread.start()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
